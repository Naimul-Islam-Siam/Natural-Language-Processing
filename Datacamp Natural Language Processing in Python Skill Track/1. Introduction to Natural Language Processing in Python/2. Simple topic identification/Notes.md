## Bag of words
- First create tokens using tokenization
- Count up the frequency of the tokens
- The more frequent a word is, the more important it might be (can be a great way of determining the significance of word in a text)

## Preprocessing
- Lowercasing letters
- Stemming/Lemmatization --> Convert to root word
- Removing stop words and unwanted tokens
- Good experiment with different approaches and then decide according to which works the best for your need

